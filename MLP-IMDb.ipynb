{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "class MLP(object):\n",
    "    def __init__(self, hidden_layers_sizes, lambda_reg=0.0001, activation='sigmoid', learning_rate=0.1, min_increment=0.000000001, max_iter=500):\n",
    "        self.hidden_layers_sizes = hidden_layers_sizes\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_increment = min_increment\n",
    "        self.max_iter = max_iter\n",
    "        self.cost_evolution = []\n",
    "        self.activation = activation\n",
    "        self.gradients = []\n",
    "        \n",
    "    def get_Y_set(self, Y):\n",
    "        Y_set = []\n",
    "        for y in Y:\n",
    "            y_l = y.tolist()\n",
    "            if y_l not in Y_set:\n",
    "                Y_set.append(y_l)\n",
    "        return Y_set\n",
    "\n",
    "    def create_matrices(self, n_in, n_out):\n",
    "        self.W = []\n",
    "        \n",
    "        num_hidden_layers = len(self.hidden_layers_sizes)\n",
    "        initial_size = n_in + 1\n",
    "        for i in range(num_hidden_layers):\n",
    "            new_W = 0.5*np.random.randn(initial_size, self.hidden_layers_sizes[i])\n",
    "            initial_size = self.hidden_layers_sizes[i] + 1\n",
    "            self.W.append(new_W)\n",
    "        final_W = 0.5*np.random.randn(initial_size, n_out)\n",
    "        self.W.append(final_W)\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        (num_elems, n_ins) = np.shape(X)\n",
    "        self.available_labels = self.get_Y_set(Y)\n",
    "        self.Y_train = self.convert_to_dummy(Y)\n",
    "        \n",
    "        self.X_train = X\n",
    "        \n",
    "        shape_Y = np.shape(self.Y_train)\n",
    "        if len(shape_Y) == 1:\n",
    "            n_out = 1\n",
    "        else:\n",
    "            n_out = shape_Y[1]\n",
    "\n",
    "        self.create_matrices(n_ins, n_out)\n",
    "        self.gradient_descend()\n",
    "\n",
    "    def convert_to_dummy(self, Y):\n",
    "        Y_dummy = []\n",
    "        size_dummy = len(self.available_labels)\n",
    "        \n",
    "        for y in Y:\n",
    "            y_l = y.tolist()\n",
    "            new_Y = [0]*size_dummy\n",
    "            ind = self.available_labels.index(y_l)\n",
    "            new_Y[ind] = 1\n",
    "            Y_dummy.append(new_Y)\n",
    "            \n",
    "        return np.array(Y_dummy)\n",
    "\n",
    "    def act(self, X):\n",
    "        if self.activation == 'relu':\n",
    "            return self.relu(X)\n",
    "        elif self.activation == 'tanh':\n",
    "            return self.tanh(X)\n",
    "        elif self.activation == 'identity':\n",
    "            return self.identity(X)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return self.sigmoid(X)\n",
    "        else:\n",
    "            raise ValueError('Unknown activation ' + self.activation)\n",
    "\n",
    "    def der_act(self, X):\n",
    "        if self.activation == 'relu':\n",
    "            return self.der_relu(X)\n",
    "        elif self.activation == 'tanh':\n",
    "            return self.der_tanh(X)\n",
    "        elif self.activation == 'identity':\n",
    "            return self.der_identity(X)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return self.der_sigmoid(X)\n",
    "        else:\n",
    "            raise ValueError('Unknown activation ' + self.activation)\n",
    "\n",
    "    def relu(self, X):\n",
    "        n_X = []\n",
    "        (num_elems, dims) = np.shape(X)\n",
    "        for i in range(num_elems):\n",
    "            new_line = []\n",
    "            for j in range(dims):\n",
    "                new_line.append(max(X[i,j], 0))\n",
    "            n_X.append(new_line)\n",
    "        return np.asarray(n_X)\n",
    "\n",
    "    def tanh(self, X):\n",
    "        return np.tanh(X)\n",
    "\n",
    "    def identity(self, X):\n",
    "        return X\n",
    "\n",
    "    def sigmoid(self, X):\n",
    "        np_X = np.array(X)\n",
    "        return 1/(1 + np.exp((-1)*np_X))\n",
    "\n",
    "    def der_relu(self, X):\n",
    "        n_X = []\n",
    "        (num_elems, dims) = np.shape(X)\n",
    "        for i in range(num_elems):\n",
    "            new_line = []\n",
    "            for j in range(dims):\n",
    "                if X[i,j] > 0:\n",
    "                    new_line.append(1)\n",
    "                else:\n",
    "                    new_line.append(0)\n",
    "            n_X.append(new_line)\n",
    "        return np.asarray(n_X)\n",
    "\n",
    "    def der_tanh(self, X):\n",
    "        t = self.tanh(X)\n",
    "        return 1 - t**2\n",
    "\n",
    "    def der_identity(self, X):\n",
    "        return np.ones(np.shape(X))\n",
    "\n",
    "    def der_sigmoid(self, X):\n",
    "        sig = self.sigmoid(X)\n",
    "        return np.multiply(sig, 1 - sig)\n",
    "    \n",
    "    def forward_with_test_W(self, X, W):\n",
    "        shape_X = np.shape(X)\n",
    "        if len(shape_X) == 1:\n",
    "            X = np.reshape(X, (1, len(X)))\n",
    "\n",
    "        activations = []\n",
    "        current_act = X\n",
    "        before_activation = X\n",
    "        num_layers = len(W)\n",
    "        for i in range(num_layers):\n",
    "            (num_elems, dims) = np.shape(current_act)\n",
    "            bias = np.ones((num_elems, 1), dtype=np.float32)\n",
    "            act_with_bias = np.append(bias, current_act, axis=1)\n",
    "            activations.append((before_activation, act_with_bias))\n",
    "            before_activation = np.dot(act_with_bias, W[i])\n",
    "            current_act = self.act(before_activation)\n",
    "        return {'output' : current_act, 'activations' : activations, 'output_before' : before_activation}\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.forward_with_test_W(X, self.W)\n",
    "\n",
    "    def diff_matrices(self, W1, W2):\n",
    "        return np.sum((W1 - W2)**2)\n",
    "    \n",
    "    def cost_with_test_W(self, W):\n",
    "        (m, d) = np.shape(self.X_train)\n",
    "        m = float(m)\n",
    "        Y_pred = np.reshape(self.forward_with_test_W(self.X_train, W)['output'], np.shape(self.Y_train))\n",
    "        first_term = (-1)*np.sum(np.multiply(self.Y_train, np.log(Y_pred)) + np.multiply(1 - self.Y_train, np.log(1 - Y_pred)))\n",
    "        reg_term = 0\n",
    "        num_layers = len(W)\n",
    "        for i in range(num_layers):\n",
    "            reg_term += np.sum(W[i]**2)\n",
    "        \n",
    "        cost_val = (1.0/(1.0*m))*(first_term) + (self.lambda_reg/(2.0*m))*reg_term\n",
    "        return cost_val\n",
    "\n",
    "    def cost(self):\n",
    "        return self.cost_with_test_W(self.W)\n",
    "    \n",
    "    def numeric_gradients(self):\n",
    "        Num_Deltas = []\n",
    "        num_layers = len(self.W)\n",
    "        test_W = []\n",
    "        for i in range(num_layers):\n",
    "            test_W.append(np.copy(self.W[i]))\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            new_grad = np.zeros(np.shape(self.W[i]))\n",
    "            pert_grad = np.zeros(np.shape(self.W[i]))\n",
    "            p = 0.0001\n",
    "            (ne,d) = np.shape(new_grad)\n",
    "            for l in range(ne):\n",
    "                for c in range(d):\n",
    "                    pert_grad[l, c] = p\n",
    "                    \n",
    "                    backup_W = np.copy(test_W[i])\n",
    "                    \n",
    "                    test_W[i] += pert_grad\n",
    "                    cost1 = self.cost_with_test_W(test_W)\n",
    "                    \n",
    "                    test_W[i] = np.copy(backup_W)\n",
    "                    test_W[i] -= pert_grad\n",
    "                    cost2 = self.cost_with_test_W(test_W)\n",
    "\n",
    "                    grad = (cost1 - cost2)/(2*p)\n",
    "                    new_grad[l,c] = grad\n",
    "\n",
    "                    pert_grad[l,c] = 0\n",
    "                    test_W[i] = np.copy(backup_W)\n",
    "                    \n",
    "            Num_Deltas.append(new_grad)\n",
    "\n",
    "        return Num_Deltas\n",
    "    \n",
    "    def predict(self, X):\n",
    "        out_raw = self.forward(X)['output']\n",
    "        sm = self.softmax(out_raw)\n",
    "        final_output = []\n",
    "        for s in sm:\n",
    "            final_output.append(self.available_labels[s])\n",
    "        return final_output\n",
    "\n",
    "    def softmax(self, Y):\n",
    "        new_Y = []\n",
    "        for y in Y:\n",
    "            exp_y = np.exp(y)\n",
    "            sum_y = np.sum(exp_y)\n",
    "            sm = exp_y/sum_y\n",
    "            new_Y.append(np.argmax(sm))\n",
    "        return new_Y\n",
    "    \n",
    "    def backprop(self):\n",
    "        Big_deltas = []\n",
    "        num_layers = len(self.W)\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            Big_deltas.append(np.zeros(np.shape(self.W[i])))\n",
    "            \n",
    "        (m, d) = np.shape(self.X_train)\n",
    "        m = float(m)\n",
    "        \n",
    "        for i, x in enumerate(self.X_train):\n",
    "            y = self.Y_train[i]\n",
    "            forw = self.forward(x)\n",
    "            pred = forw['output']\n",
    "            pred_before = forw['output_before']\n",
    "            pred_der = np.ones(np.shape(np.transpose(self.der_act(pred_before))))\n",
    "            activations = forw['activations']\n",
    "\n",
    "            final_delta = np.multiply(np.transpose(pred - y), pred_der)\n",
    "            last_delta = final_delta\n",
    "            num_activations = len(activations)\n",
    "\n",
    "            for j in range(num_activations - 1):\n",
    "                curr_out = activations[num_activations - j - 1][0]\n",
    "                curr_act_with_bias = activations[num_activations - j -1][1]\n",
    "\n",
    "                Big_deltas[num_layers - j - 1] += np.dot(np.transpose(curr_act_with_bias), np.transpose(last_delta))\n",
    "\n",
    "                der = np.transpose(self.der_act(curr_out))\n",
    "                small_W = self.W[num_layers - j -1][1:, :]\n",
    "                \n",
    "                new_delta = np.multiply(np.dot(small_W, last_delta), der)\n",
    "\n",
    "                last_delta = new_delta\n",
    "            curr_act_with_bias = activations[0][1]\n",
    "            Big_deltas[0] += np.dot(np.transpose(curr_act_with_bias), np.transpose(last_delta))\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            reg_term = np.copy(self.W[i])\n",
    "            reg_term[:,0] *= 0\n",
    "            Big_deltas[i] += self.lambda_reg*reg_term\n",
    "            Big_deltas[i] *= (1.0/m)\n",
    "\n",
    "        return Big_deltas\n",
    "\n",
    "    def one_step_grad_desc(self):\n",
    "        Deltas = self.backprop()\n",
    "        Deltas_num = self.numeric_gradients()\n",
    "\n",
    "        self.gradients.append([Deltas, Deltas_num])\n",
    "        \n",
    "        num_layers = len(self.W)\n",
    "        for i in range(num_layers):\n",
    "            self.W[i] -= self.learning_rate*Deltas[i]\n",
    "            \n",
    "\n",
    "    def make_W_with_sklearn_data(self, coefs_, intercepts_, classes_):\n",
    "        num_matrices = len(coefs_)\n",
    "        self.W = []\n",
    "        for i in range(num_matrices):\n",
    "            new_W = np.vstack((intercepts_[i], coefs_[i]))\n",
    "            self.W.append(new_W)\n",
    "        self.available_labels = classes_.tolist()\n",
    "\n",
    "    def make_mlp(self, mlp):\n",
    "        self.make_W_with_sklearn_data(mlp.coefs_, mlp.intercepts_, mlp.classes_)\n",
    "        \n",
    "    def gradient_descend(self):\n",
    "        last_cost = self.cost()\n",
    "        self.cost_evolution = [last_cost]\n",
    "        self.gradients = []\n",
    "        \n",
    "        has_exited_early = False\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            print(i)\n",
    "            self.one_step_grad_desc()\n",
    "            next_cost = self.cost()\n",
    "            self.cost_evolution.append(next_cost)\n",
    "            \n",
    "            cost_update = next_cost - last_cost\n",
    "            \n",
    "            if abs(cost_update) < self.min_increment:\n",
    "                has_exited_early = True\n",
    "                print('Exited gradient descend by min increment')\n",
    "                break\n",
    "            last_cost = next_cost\n",
    "        if not has_exited_early:\n",
    "            print('Exited gradient descend by max iterations')\n",
    "\n",
    "        diffs = []\n",
    "        for g in self.gradients:\n",
    "            back = g[0]\n",
    "            num = g[1]\n",
    "            for i, m in enumerate(back):\n",
    "                new_diff = self.diff_matrices(m, num[i])\n",
    "                diffs.append(new_diff)\n",
    "        print('Differeces', diffs)\n",
    "        print('mean difference', np.mean(diffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier((5,), activation='tanh', max_iter=200)\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MLP((5,), activation='sigmoid', max_iter=200)\n",
    "m.make_mlp(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_m = m.predict(X_test)\n",
    "\n",
    "acc_final = accuracy_score(y_test, predictions_m)\n",
    "print(\"Accuracy: \", acc_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
